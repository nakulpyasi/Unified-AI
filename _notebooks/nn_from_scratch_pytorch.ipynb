{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "# Same as numpy\n",
    "# The main diff is pytorch works on a gpu\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.arange(58,387,step = 5.5, dtype=np.float32).reshape(10,6)\n",
    "inp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = np.arange(6000,9000,step = 320.7,dtype=np.float32)\n",
    "actual.dtype\n",
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything to torch\n",
    "inp= torch.from_numpy(inp)\n",
    "actual = torch.from_numpy(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the wt's and bias\n",
    "# requires grad will make sure that gradients will be automatically calculated for the tensor\n",
    "# Define the datatypes to match the input and output tesnsor\n",
    "wt = torch.randn(6, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.dtype,bias.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model formula\n",
    "def model(inp):\n",
    "    return inp @ wt.t() + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12.7866, 13.0640, 13.3413, 13.6186, 13.8960, 14.1733, 14.4507, 14.7280,\n",
       "         15.0053, 15.2827], grad_fn=<AddBackward0>),\n",
       " '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++',\n",
       " tensor([[ 58.0000,  63.5000,  69.0000,  74.5000,  80.0000,  85.5000],\n",
       "         [ 91.0000,  96.5000, 102.0000, 107.5000, 113.0000, 118.5000],\n",
       "         [124.0000, 129.5000, 135.0000, 140.5000, 146.0000, 151.5000],\n",
       "         [157.0000, 162.5000, 168.0000, 173.5000, 179.0000, 184.5000],\n",
       "         [190.0000, 195.5000, 201.0000, 206.5000, 212.0000, 217.5000],\n",
       "         [223.0000, 228.5000, 234.0000, 239.5000, 245.0000, 250.5000],\n",
       "         [256.0000, 261.5000, 267.0000, 272.5000, 278.0000, 283.5000],\n",
       "         [289.0000, 294.5000, 300.0000, 305.5000, 311.0000, 316.5000],\n",
       "         [322.0000, 327.5000, 333.0000, 338.5000, 344.0000, 349.5000],\n",
       "         [355.0000, 360.5000, 366.0000, 371.5000, 377.0000, 382.5000]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of model calculation\n",
    "pred = model(inp)\n",
    "pred, '+'*100 , inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(56038804., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation of the loss function - mse\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "# calculation of loss considering random wts and bias is initialized\n",
    "loss = mse(pred,actual)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8699,  0.5496, -0.3244,  0.2490,  0.4581, -0.0539],\n",
      "       requires_grad=True)\n",
      "tensor([-3242695.5000, -3324415.5000, -3406136.0000, -3487856.2500,\n",
      "        -3569576.5000, -3651296.7500])\n",
      "tensor([-14858.2334])\n"
     ]
    }
   ],
   "source": [
    "print(wt)\n",
    "print(wt.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(56038804., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -ve gradient - increase wt - less loss\n",
    "# -ve gradient - decrease wt - more loss\n",
    "\n",
    "# +ve gradient - increase wt - more loss\n",
    "# +ve gradient - decrease wt - less loss\n",
    "\n",
    "# Increase and decrease of loss is proportional to the gradient of the loss\n",
    "\n",
    "# torch.no_grad that the gradient is not calculated while updating the wt's and the bias\n",
    "\n",
    "# 1e-5 is the learning rate to make sure we move slowly towards the minimum of cost function\n",
    "\n",
    "with torch.no_grad():\n",
    "    wt -= wt.grad * 1e-5\n",
    "    bias -= bias.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_zero_() makes sure that the gradients are not added to .grad and are made equal to 0\n",
    "wt.grad.zero_()\n",
    "bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14930.9082, 21756.2363, 28581.5684, 35406.8984, 42232.2305, 49057.5586,\n",
      "        55882.8867, 62708.2148, 69533.5469, 76358.8672],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor(1.8084e+09, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Loss calcuation to check if the loss improves after gradient descent\n",
    "\n",
    "preds = model(inp)\n",
    "print(preds)\n",
    "\n",
    "loss = mse(preds, actual)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wt -= wt.grad * 1e-5\n",
    "    bias -= bias.grad * 1e-5\n",
    "    wt.grad.zero_()\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31.5571, 33.7937, 33.7370, 35.1276, 36.1538, 36.4590],\n",
      "       requires_grad=True)\n",
      "tensor([0.2911], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(wt)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8084e+09, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inp)\n",
    "loss = mse(preds, actual)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1706e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.1551e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.1398e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.1247e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.1098e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0951e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0806e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0663e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0523e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0384e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0247e+08, grad_fn=<DivBackward0>)\n",
      "tensor(1.0112e+08, grad_fn=<DivBackward0>)\n",
      "tensor(99783600., grad_fn=<DivBackward0>)\n",
      "tensor(98469440., grad_fn=<DivBackward0>)\n",
      "tensor(97173368., grad_fn=<DivBackward0>)\n",
      "tensor(95895144., grad_fn=<DivBackward0>)\n",
      "tensor(94634504., grad_fn=<DivBackward0>)\n",
      "tensor(93391240., grad_fn=<DivBackward0>)\n",
      "tensor(92165096., grad_fn=<DivBackward0>)\n",
      "tensor(90955816., grad_fn=<DivBackward0>)\n",
      "tensor(89763208., grad_fn=<DivBackward0>)\n",
      "tensor(88587008., grad_fn=<DivBackward0>)\n",
      "tensor(87426992., grad_fn=<DivBackward0>)\n",
      "tensor(86282960., grad_fn=<DivBackward0>)\n",
      "tensor(85154672., grad_fn=<DivBackward0>)\n",
      "tensor(84041944., grad_fn=<DivBackward0>)\n",
      "tensor(82944512., grad_fn=<DivBackward0>)\n",
      "tensor(81862200., grad_fn=<DivBackward0>)\n",
      "tensor(80794792., grad_fn=<DivBackward0>)\n",
      "tensor(79742064., grad_fn=<DivBackward0>)\n",
      "tensor(78703856., grad_fn=<DivBackward0>)\n",
      "tensor(77679928., grad_fn=<DivBackward0>)\n",
      "tensor(76670096., grad_fn=<DivBackward0>)\n",
      "tensor(75674192., grad_fn=<DivBackward0>)\n",
      "tensor(74691976., grad_fn=<DivBackward0>)\n",
      "tensor(73723280., grad_fn=<DivBackward0>)\n",
      "tensor(72767904., grad_fn=<DivBackward0>)\n",
      "tensor(71825728., grad_fn=<DivBackward0>)\n",
      "tensor(70896488., grad_fn=<DivBackward0>)\n",
      "tensor(69980080., grad_fn=<DivBackward0>)\n",
      "tensor(69076256., grad_fn=<DivBackward0>)\n",
      "tensor(68184896., grad_fn=<DivBackward0>)\n",
      "tensor(67305792., grad_fn=<DivBackward0>)\n",
      "tensor(66438804., grad_fn=<DivBackward0>)\n",
      "tensor(65583744., grad_fn=<DivBackward0>)\n",
      "tensor(64740472., grad_fn=<DivBackward0>)\n",
      "tensor(63908812., grad_fn=<DivBackward0>)\n",
      "tensor(63088596., grad_fn=<DivBackward0>)\n",
      "tensor(62279680., grad_fn=<DivBackward0>)\n",
      "tensor(61481900., grad_fn=<DivBackward0>)\n",
      "tensor(60695092., grad_fn=<DivBackward0>)\n",
      "tensor(59919096., grad_fn=<DivBackward0>)\n",
      "tensor(59153812., grad_fn=<DivBackward0>)\n",
      "tensor(58399072., grad_fn=<DivBackward0>)\n",
      "tensor(57654720., grad_fn=<DivBackward0>)\n",
      "tensor(56920600., grad_fn=<DivBackward0>)\n",
      "tensor(56196588., grad_fn=<DivBackward0>)\n",
      "tensor(55482560., grad_fn=<DivBackward0>)\n",
      "tensor(54778368., grad_fn=<DivBackward0>)\n",
      "tensor(54083864., grad_fn=<DivBackward0>)\n",
      "tensor(53398920., grad_fn=<DivBackward0>)\n",
      "tensor(52723412., grad_fn=<DivBackward0>)\n",
      "tensor(52057192., grad_fn=<DivBackward0>)\n",
      "tensor(51400168., grad_fn=<DivBackward0>)\n",
      "tensor(50752156., grad_fn=<DivBackward0>)\n",
      "tensor(50113088., grad_fn=<DivBackward0>)\n",
      "tensor(49482808., grad_fn=<DivBackward0>)\n",
      "tensor(48861220., grad_fn=<DivBackward0>)\n",
      "tensor(48248176., grad_fn=<DivBackward0>)\n",
      "tensor(47643592., grad_fn=<DivBackward0>)\n",
      "tensor(47047312., grad_fn=<DivBackward0>)\n",
      "tensor(46459252., grad_fn=<DivBackward0>)\n",
      "tensor(45879276., grad_fn=<DivBackward0>)\n",
      "tensor(45307292., grad_fn=<DivBackward0>)\n",
      "tensor(44743192., grad_fn=<DivBackward0>)\n",
      "tensor(44186856., grad_fn=<DivBackward0>)\n",
      "tensor(43638184., grad_fn=<DivBackward0>)\n",
      "tensor(43097064., grad_fn=<DivBackward0>)\n",
      "tensor(42563388., grad_fn=<DivBackward0>)\n",
      "tensor(42037064., grad_fn=<DivBackward0>)\n",
      "tensor(41517988., grad_fn=<DivBackward0>)\n",
      "tensor(41006060., grad_fn=<DivBackward0>)\n",
      "tensor(40501188., grad_fn=<DivBackward0>)\n",
      "tensor(40003256., grad_fn=<DivBackward0>)\n",
      "tensor(39512176., grad_fn=<DivBackward0>)\n",
      "tensor(39027864., grad_fn=<DivBackward0>)\n",
      "tensor(38550220., grad_fn=<DivBackward0>)\n",
      "tensor(38079148., grad_fn=<DivBackward0>)\n",
      "tensor(37614560., grad_fn=<DivBackward0>)\n",
      "tensor(37156376., grad_fn=<DivBackward0>)\n",
      "tensor(36704496., grad_fn=<DivBackward0>)\n",
      "tensor(36258836., grad_fn=<DivBackward0>)\n",
      "tensor(35819320., grad_fn=<DivBackward0>)\n",
      "tensor(35385848., grad_fn=<DivBackward0>)\n",
      "tensor(34958348., grad_fn=<DivBackward0>)\n",
      "tensor(34536732., grad_fn=<DivBackward0>)\n",
      "tensor(34120928., grad_fn=<DivBackward0>)\n",
      "tensor(33710836., grad_fn=<DivBackward0>)\n",
      "tensor(33306410., grad_fn=<DivBackward0>)\n",
      "tensor(32907540., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    preds = model(inp)\n",
    "    loss = mse(preds, actual)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        wt -= wt.grad * 1e-8\n",
    "        bias -= bias.grad * 1e-8\n",
    "        wt.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32514160., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inp)\n",
    "loss = mse(preds, actual)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3773.3501,  5489.8921,  7206.4336,  8922.9756, 10639.5166, 12356.0586,\n",
       "        14072.6006, 15789.1426, 17505.6836, 19222.2246],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6000.0000, 6320.7002, 6641.4004, 6962.1006, 7282.8008, 7603.5010,\n",
       "        7924.2012, 8244.9014, 8565.6016, 8886.3018])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6338af0e41d49d4a98e031440c1e6ba1901a7edf2fea4c64b6b2d7a0b97a576b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
