{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent from Scratch using Pytorch\n",
    "> A tutorial of gradient descent from scratch using pytorch \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/nn_image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/nn_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of a gradient descent using PYTORCH\n",
    "\n",
    "\n",
    "`gradient_descent` is the foundation of all deep learning problems. It is super critical tp understand this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the implementation of a neural network from scratch with even lesser steps using pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import  torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 58. ,  63.5,  69. ,  74.5,  80. ,  85.5],\n",
       "       [ 91. ,  96.5, 102. , 107.5, 113. , 118.5],\n",
       "       [124. , 129.5, 135. , 140.5, 146. , 151.5],\n",
       "       [157. , 162.5, 168. , 173.5, 179. , 184.5],\n",
       "       [190. , 195.5, 201. , 206.5, 212. , 217.5],\n",
       "       [223. , 228.5, 234. , 239.5, 245. , 250.5],\n",
       "       [256. , 261.5, 267. , 272.5, 278. , 283.5],\n",
       "       [289. , 294.5, 300. , 305.5, 311. , 316.5],\n",
       "       [322. , 327.5, 333. , 338.5, 344. , 349.5],\n",
       "       [355. , 360.5, 366. , 371.5, 377. , 382.5]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "inp = np.arange(58,387,step = 5.5, dtype=np.float32).reshape(10,6)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), (10, 6))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "inp.dtype, inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6000.    ],\n",
       "       [6320.7   ],\n",
       "       [6641.4004],\n",
       "       [6962.1006],\n",
       "       [7282.801 ],\n",
       "       [7603.501 ],\n",
       "       [7924.201 ],\n",
       "       [8244.901 ],\n",
       "       [8565.602 ],\n",
       "       [8886.302 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "actual = np.arange(6000,9000,step = 320.7,dtype=np.float32).reshape(10,1)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), (10, 1))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "actual.dtype, actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= torch.from_numpy(inp)\n",
    "actual = torch.from_numpy(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.TensorDataset,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset gives us a tuple of inputs and targets\n",
    "ds = TensorDataset(inp, actual)\n",
    "\n",
    "# To divide the data into train and validation set we use random_split\n",
    "# [8,2] splits the data into training and validation dataset\n",
    "#tr_dataset, val_dataset = random_split(ds,[8,2])\n",
    "\n",
    "# Dataloader helps to split data into batches and shuffling the data\n",
    "train_loader = DataLoader(ds, shuffle=True)\n",
    "\n",
    "# datatypes of dataset an train_loader\n",
    "type(ds) , type(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dataloader always gives a tuple of the training data and the label with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[190.0000, 195.5000, 201.0000, 206.5000, 212.0000, 217.5000]])\n",
      "tensor([[7282.8008]])\n",
      "tensor([[322.0000, 327.5000, 333.0000, 338.5000, 344.0000, 349.5000]])\n",
      "tensor([[8565.6016]])\n",
      "tensor([[ 91.0000,  96.5000, 102.0000, 107.5000, 113.0000, 118.5000]])\n",
      "tensor([[6320.7002]])\n",
      "tensor([[256.0000, 261.5000, 267.0000, 272.5000, 278.0000, 283.5000]])\n",
      "tensor([[7924.2012]])\n",
      "tensor([[124.0000, 129.5000, 135.0000, 140.5000, 146.0000, 151.5000]])\n",
      "tensor([[6641.4004]])\n",
      "tensor([[289.0000, 294.5000, 300.0000, 305.5000, 311.0000, 316.5000]])\n",
      "tensor([[8244.9014]])\n",
      "tensor([[58.0000, 63.5000, 69.0000, 74.5000, 80.0000, 85.5000]])\n",
      "tensor([[6000.]])\n",
      "tensor([[355.0000, 360.5000, 366.0000, 371.5000, 377.0000, 382.5000]])\n",
      "tensor([[8886.3018]])\n",
      "tensor([[157.0000, 162.5000, 168.0000, 173.5000, 179.0000, 184.5000]])\n",
      "tensor([[6962.1006]])\n",
      "tensor([[223.0000, 228.5000, 234.0000, 239.5000, 245.0000, 250.5000]])\n",
      "tensor([[7603.5010]])\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "#Iteration to see what a dataloader provides\n",
    "for data,label in train_loader:\n",
    "    print(data)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using NN.Linear and pass the input through it to generate to produce the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.linear automatically creates the matrix of wt abd bias\n",
    "# inputs are the number of columns in a tabular dataset\n",
    "# outputs can be the number of outputs\n",
    "input_size = 6\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size,output_size)\n",
    "\n",
    "# To look at the weight and bias use a parameter method\n",
    "list(model.parameters())\n",
    "\n",
    "# using the model to generate predictions\n",
    "predictions = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 1e-06\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "# Now the loss function can be computed by directly using F.mse_loss\n",
    "loss = F.mse_loss(predictions,actual)\n",
    "\n",
    "# Models wts and bias can be updated automatically using a optimizer\n",
    "opt = torch.optim.SGD(model.parameters(),lr = 1e-6)\n",
    "\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the process of `Gradient_Descent` to find optimal weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1276.5966, grad_fn=<MseLossBackward0>)\n",
      "tensor(2668.6709, grad_fn=<MseLossBackward0>)\n",
      "tensor(7169.9878, grad_fn=<MseLossBackward0>)\n",
      "tensor(469.6278, grad_fn=<MseLossBackward0>)\n",
      "tensor(6568.8335, grad_fn=<MseLossBackward0>)\n",
      "tensor(1190.0479, grad_fn=<MseLossBackward0>)\n",
      "tensor(325.2668, grad_fn=<MseLossBackward0>)\n",
      "tensor(5114.6245, grad_fn=<MseLossBackward0>)\n",
      "tensor(1722.8174, grad_fn=<MseLossBackward0>)\n",
      "tensor(1077.5576, grad_fn=<MseLossBackward0>)\n",
      "tensor(160.6259, grad_fn=<MseLossBackward0>)\n",
      "tensor(375.2014, grad_fn=<MseLossBackward0>)\n",
      "tensor(4391.7803, grad_fn=<MseLossBackward0>)\n",
      "tensor(2078.6121, grad_fn=<MseLossBackward0>)\n",
      "tensor(2570.0146, grad_fn=<MseLossBackward0>)\n",
      "tensor(725.0766, grad_fn=<MseLossBackward0>)\n",
      "tensor(2081.1060, grad_fn=<MseLossBackward0>)\n",
      "tensor(873.6527, grad_fn=<MseLossBackward0>)\n",
      "tensor(83.2389, grad_fn=<MseLossBackward0>)\n",
      "tensor(1916.8406, grad_fn=<MseLossBackward0>)\n",
      "tensor(719.1458, grad_fn=<MseLossBackward0>)\n",
      "tensor(1902.9702, grad_fn=<MseLossBackward0>)\n",
      "tensor(42.3262, grad_fn=<MseLossBackward0>)\n",
      "tensor(763.7343, grad_fn=<MseLossBackward0>)\n",
      "tensor(365.8590, grad_fn=<MseLossBackward0>)\n",
      "tensor(112.0207, grad_fn=<MseLossBackward0>)\n",
      "tensor(925.1934, grad_fn=<MseLossBackward0>)\n",
      "tensor(158.8609, grad_fn=<MseLossBackward0>)\n",
      "tensor(363.2858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4345, grad_fn=<MseLossBackward0>)\n",
      "tensor(453.9729, grad_fn=<MseLossBackward0>)\n",
      "tensor(123.0280, grad_fn=<MseLossBackward0>)\n",
      "tensor(30.7137, grad_fn=<MseLossBackward0>)\n",
      "tensor(350.9402, grad_fn=<MseLossBackward0>)\n",
      "tensor(89.2324, grad_fn=<MseLossBackward0>)\n",
      "tensor(296.6872, grad_fn=<MseLossBackward0>)\n",
      "tensor(19.0595, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.8788, grad_fn=<MseLossBackward0>)\n",
      "tensor(83.6939, grad_fn=<MseLossBackward0>)\n",
      "tensor(173.1255, grad_fn=<MseLossBackward0>)\n",
      "tensor(155.1168, grad_fn=<MseLossBackward0>)\n",
      "tensor(125.8494, grad_fn=<MseLossBackward0>)\n",
      "tensor(179.1650, grad_fn=<MseLossBackward0>)\n",
      "tensor(92.3213, grad_fn=<MseLossBackward0>)\n",
      "tensor(23.1508, grad_fn=<MseLossBackward0>)\n",
      "tensor(34.4067, grad_fn=<MseLossBackward0>)\n",
      "tensor(54.5636, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.6440, grad_fn=<MseLossBackward0>)\n",
      "tensor(145.8339, grad_fn=<MseLossBackward0>)\n",
      "tensor(47.0510, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.9315, grad_fn=<MseLossBackward0>)\n",
      "tensor(59.0601, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9200, grad_fn=<MseLossBackward0>)\n",
      "tensor(27.8658, grad_fn=<MseLossBackward0>)\n",
      "tensor(23.2401, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5090, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5459, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6879, grad_fn=<MseLossBackward0>)\n",
      "tensor(32.2145, grad_fn=<MseLossBackward0>)\n",
      "tensor(31.1918, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8559, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.7257, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5545, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6442, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6892, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4676, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "# Steps for nn implementation\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    for data,label in train_loader:\n",
    "        # preds\n",
    "        preds = model(data)\n",
    "        # calculate loss\n",
    "        loss = F.mse_loss(preds,label)\n",
    "        # gradient calculation\n",
    "        loss.backward()\n",
    "        # update wts and bias wrt loss\n",
    "        opt.step()\n",
    "        # gradients values are 0 again\n",
    "        opt.zero_grad()\n",
    "    if epoch%30==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can compare the results obtained by the model after the weights are updated and `gradient_descent` is run\n",
    "\n",
    "We can see that we are quite close to the actual value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5996.6123],\n",
       "        [6318.0566],\n",
       "        [6639.5049],\n",
       "        [6960.9502],\n",
       "        [7282.3916],\n",
       "        [7603.8350],\n",
       "        [7925.2881],\n",
       "        [8246.7295],\n",
       "        [8568.1748],\n",
       "        [8889.6162]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "# predictions after the weights are updated\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6000.0000],\n",
       "        [6320.7002],\n",
       "        [6641.4004],\n",
       "        [6962.1006],\n",
       "        [7282.8008],\n",
       "        [7603.5010],\n",
       "        [7924.2012],\n",
       "        [8244.9014],\n",
       "        [8565.6016],\n",
       "        [8886.3018]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-\n",
    "# Actual values from the model\n",
    "actual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6338af0e41d49d4a98e031440c1e6ba1901a7edf2fea4c64b6b2d7a0b97a576b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
