{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset - NN example(Pytorch)\n",
    "> A tutorial of gradient descent from scratch using pytorch \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/MNIST.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, int, torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse- output\n",
    "# this line of code will download the images(60000) in the defined root folder\n",
    "# download = True - when ran once will nor re-download all the images if it finds the images in the root path\n",
    "# train = True means it is the training set\n",
    "dataset = MNIST(root='Deep_Learning_Explorations/data/',train = True, download=True,transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset is a tuple of image and the label\n",
    "# indexing the dataset will show us that\n",
    "# shape of thwe image is 1*28*28\n",
    "# 28 * 28 are the pixel values ranging from values 0 to 1\n",
    "# this image has a just one channel as it is  a gray scale image\n",
    "type(dataset[0][0]), type(dataset[0][1]), dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0], cmap = 'gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset - Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "# Split the dataset into a training and the validation set\n",
    "tr_data, val_data = random_split(dataset,[50000,10000])\n",
    "len(tr_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# Dataloader helps in converting the dataset into batches of data by describing the batch_size\n",
    "tr_loader = DataLoader(tr_data,batch_size=200,shuffle=True)\n",
    "val_loader = DataLoader(val_data,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 28, 28])\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "# We can see that the data is a batch of 128 images and 120 labels\n",
    "for data,label in tr_loader:\n",
    "    print(data.shape)\n",
    "    print(len(label))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "# x@w.t()+ bias - Use the same equation\n",
    "model = nn.Linear(in_features= 784,out_features=10)\n",
    "\n",
    "# A model will randomly initialize the parameters(wt's and biases)\n",
    "model.weight.shape, model.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 28, 28])\n",
      "torch.Size([200, 784])\n",
      "torch.Size([200, 10])\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "for img,label in tr_loader:\n",
    "    # shape of the batch in the beggining\n",
    "    print(img.shape)\n",
    "    #this functionality can be added inside the model in the forward method\n",
    "    img = img.reshape(-1,28*28)\n",
    "    # shape after reshaping into a vector of 784 elements\n",
    "    print(img.shape)\n",
    "    # applying the model to the reshaped img\n",
    "    pred = model(img)\n",
    "    print(model(img).shape)\n",
    "    # As we can see that the model has outputted 10 probabilities which is a prob for all elements from 0-9\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1780, -0.2030, -0.1832,  ..., -0.0791,  0.1601, -0.0289],\n",
      "        [-0.0835, -0.3761, -0.2121,  ..., -0.2008, -0.2714, -0.2222],\n",
      "        [-0.0419, -0.1072, -0.0031,  ..., -0.1503,  0.0071, -0.1969],\n",
      "        ...,\n",
      "        [-0.1106, -0.1116,  0.1379,  ..., -0.0675, -0.0677,  0.0206],\n",
      "        [-0.0918, -0.1213, -0.0014,  ..., -0.1266,  0.1137,  0.3449],\n",
      "        [-0.2163,  0.0296,  0.0380,  ..., -0.0783, -0.1645, -0.1849]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "print(pred)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functionality to our NN model\n",
    "\n",
    "To add additional functionality to the NN model we need to create a MnistModel class and inheret the nn.Module. \n",
    "Addiditional functionality in this is the step of reshaping the batch of data passing through the model to a vector of 784 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional functionality to the NN model\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]), torch.Size([10]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of wts and bias\n",
    "model.layer.weight.shape, model.layer.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofmax\n",
    "\n",
    "Softmax converts a vector of K real numbers into a probability distribution of K possible outcomes\n",
    "\n",
    "It is basically calculated by taking the exponent of preds and dividing by their sum to make sure its sum is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 10])\n",
      "torch.Size([200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/0sh22ln930vdhyh1wkttl89m0000gp/T/ipykernel_24095/3939508011.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.sum(F.softmax(pred[0])).item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([200]), torch.Size([200]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the model from the above class works\n",
    "for dta,label in tr_loader:\n",
    "    pred = model(dta)\n",
    "    print(pred.shape)\n",
    "    print(label.shape)\n",
    "    break\n",
    "\n",
    "# We will apply softmax now - which converts the probability b/w 0 and 1 and the sum is 1\n",
    "torch.sum(F.softmax(pred[0])).item()\n",
    "\n",
    "# Applying softmax on the whole batch\n",
    "pred_s = F.softmax(pred,dim=1)\n",
    "\n",
    "# torch amx function gives us the index of the max probability as well as the probability\n",
    "index_prob,prob = torch.max(pred_s,dim=1)\n",
    "\n",
    "index_prob.shape,prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "The predictions are converted into probabilities and the highest probability is calculated using the max function\n",
    "\n",
    "The index of the highest probability is then compared to the actual label and the accuracy % is calculated by divding the correct predictions with the total images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0700)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def metric_acc(out,label):\n",
    "    index_prob,prob = torch.max(pred_s,dim=1)\n",
    "    return torch.sum(prob==label)/prob.numel()\n",
    "\n",
    "# We will get the same value even if we do not apply the softmax as\n",
    "# e^x is an increasing function, i.e., if y1 > y2, then e^y1 > e^y2. The same holds after averaging out the values to get the softmax.\n",
    "metric_acc(prob,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3289, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross entropy -ve log of predicted prob\n",
    "# loss function will be cross_entropy\n",
    "loss = F.cross_entropy(pred, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This fit function is the training step. This training step invovles training the model on the training dataloader, calculating the loss, calculating the gradient for the\n",
    "train loader and updating the weights and reseting the gradient at the end.\n",
    "\n",
    "For the second part of the loop - we validate the model on the validation dataloader. The steps include calculating the loss and accuracy after each epoch and printing them at\n",
    "the end. We can notice that the loss and the accuracy on the validation set improves after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,learning_rate,model,train_loader,val_loader,opt_func=torch.optim.SGD):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate , momentum=0.9)\n",
    "    out_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training step on the training dataloader\n",
    "        for batch in tr_loader:\n",
    "            #extract batch of images and label\n",
    "            img,label = batch\n",
    "            #calculate prediction using the MNISTMODEL class initialized above\n",
    "            pred = model(img)\n",
    "            #Since this is a multi-label image classification model- the loss function is cross entropy\n",
    "            loss = F.cross_entropy(pred,label)\n",
    "            # In this step we calculate the gradient of the loss function with respect to the parameters or 784 pixels in this case\n",
    "            loss.backward()\n",
    "            # In this step we update the weights\n",
    "            optimizer.step()\n",
    "            # We make the gradient zero again so that now the gradients are not calculated untill the training is not done\n",
    "            optimizer.zero_grad()\n",
    "         \n",
    "        # Validation on the validation dataloader   \n",
    "        for batch_val in val_loader:\n",
    "            img_val,label_val = batch_val\n",
    "            pred_val = model(img_val)\n",
    "            # loss is computed\n",
    "            loss_val = F.cross_entropy(pred_val,label_val)\n",
    "            # Accuracy is computed\n",
    "            acc_val = metric_acc(pred_val,label_val)\n",
    "            out_lst.append({'Epoch':epoch,'val_loss': loss_val, 'val_acc': acc_val})\n",
    "        \n",
    "        # Accuracies and loss are stacked together for each epoch, mean is calculated and the results are printed    \n",
    "        val_loss_epoch = torch.stack([dct['val_loss'] for dct in out_lst]).mean()\n",
    "        val_acc_epoch = torch.stack([dct['val_acc'] for dct in out_lst]).mean()                  \n",
    "        print('Epoch: {0}, loss: {1}, accuracy_val: {2}'.format(epoch,val_loss_epoch,val_acc_epoch))\n",
    "\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that with each epoch the loss and accuracies improve.\n",
    "You can try to use different number of epochs and learning rate to see if you can improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.5632244348526001, accuracy_val: 0.861500084400177\n",
      "Epoch: 1, loss: 0.5117447376251221, accuracy_val: 0.8712499737739563\n",
      "Epoch: 2, loss: 0.4806540310382843, accuracy_val: 0.8765000700950623\n",
      "Epoch: 3, loss: 0.45891621708869934, accuracy_val: 0.8803249597549438\n",
      "Epoch: 4, loss: 0.44262486696243286, accuracy_val: 0.8833799362182617\n",
      "Epoch: 5, loss: 0.4297642111778259, accuracy_val: 0.885816752910614\n",
      "Epoch: 6, loss: 0.4192468225955963, accuracy_val: 0.8878857493400574\n",
      "Epoch: 7, loss: 0.41042187809944153, accuracy_val: 0.889549970626831\n",
      "Epoch: 8, loss: 0.4029468595981598, accuracy_val: 0.891033411026001\n",
      "Epoch: 9, loss: 0.3964667320251465, accuracy_val: 0.8922899961471558\n",
      "Epoch: 10, loss: 0.3907521069049835, accuracy_val: 0.8933908939361572\n",
      "Epoch: 11, loss: 0.385611355304718, accuracy_val: 0.8943833708763123\n",
      "Epoch: 12, loss: 0.38104525208473206, accuracy_val: 0.8952845931053162\n",
      "Epoch: 13, loss: 0.37690234184265137, accuracy_val: 0.8960857391357422\n",
      "Epoch: 14, loss: 0.37316834926605225, accuracy_val: 0.8968601226806641\n",
      "Epoch: 15, loss: 0.3697192370891571, accuracy_val: 0.8975686430931091\n",
      "Epoch: 16, loss: 0.36654046177864075, accuracy_val: 0.8981589078903198\n",
      "Epoch: 17, loss: 0.3636338710784912, accuracy_val: 0.8987833857536316\n",
      "Epoch: 18, loss: 0.3609280586242676, accuracy_val: 0.8993525505065918\n",
      "Epoch: 19, loss: 0.35844382643699646, accuracy_val: 0.899869978427887\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "fit(20,learning_rate=0.005,model= model,train_loader = tr_loader,val_loader = val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = MNIST(root='Deep_Learning_Explorations/data/',train = False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can plot the gray scale image using matplotlib and specifying cmap = gray\n",
    "# gray scale has just 1 channnel\n",
    "# 1*28*28, 1 here specifies that image has just one channel\n",
    "img, label = img_test[12]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_function(img,model):\n",
    "    img.shape\n",
    "    inp = img.unsqueeze(0)\n",
    "    out = model(inp)\n",
    "    prob , preds = torch.max(out,dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3 , Predicted: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = img_test[200]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', pred_function(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6 , Predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = img_test[123]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', pred_function(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on all images - the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the test set\n",
    "test_loader = DataLoader(img_test, batch_size=20)\n",
    "\n",
    "# pass the batches to the model and calculate model accuracy and loss\n",
    "def test_set_preds(test_loader,model):\n",
    "    out_lst = []\n",
    "    for i, batch_val in enumerate(test_loader):\n",
    "            img_val,label_val = batch_val\n",
    "            pred_val = model(img_val)\n",
    "            # loss is computed for that batch\n",
    "            loss_val = F.cross_entropy(pred_val,label_val)\n",
    "            # Accuracy is computed for that batch\n",
    "            acc_val = metric_acc(pred_val,label_val)\n",
    "            out_lst.append({'val_loss': loss_val, 'val_acc': acc_val})     \n",
    "    #Accuracies and loss are stacked together for each epoch, mean is calculated and the results are printed    \n",
    "    final_loss = torch.stack([dct['val_loss'] for dct in out_lst]).mean()\n",
    "    final_acc= torch.stack([dct['val_acc'] for dct in out_lst]).mean()             \n",
    "    return 'loss: {0}, accuracy_val: {1}'.format(final_loss,final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss: 0.297660231590271, accuracy_val: 0.9175999164581299'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_preds(test_loader,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6338af0e41d49d4a98e031440c1e6ba1901a7edf2fea4c64b6b2d7a0b97a576b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
