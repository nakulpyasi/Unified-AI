{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent from Scratch using\n",
    "> A tutorial of gradient descent from scratch using pytorch \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/nn_image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook is an implementation of a gradient descent using PYTORCH\n",
    "\n",
    "\n",
    "`gradient_descent` is the foundation of all deep learning problems. It is super critical tp understand this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the implementation of a neural network from scratch with even lesser steps using pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import  torch.nn as nn√ü\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 58. ,  63.5,  69. ,  74.5,  80. ,  85.5],\n",
       "       [ 91. ,  96.5, 102. , 107.5, 113. , 118.5],\n",
       "       [124. , 129.5, 135. , 140.5, 146. , 151.5],\n",
       "       [157. , 162.5, 168. , 173.5, 179. , 184.5],\n",
       "       [190. , 195.5, 201. , 206.5, 212. , 217.5],\n",
       "       [223. , 228.5, 234. , 239.5, 245. , 250.5],\n",
       "       [256. , 261.5, 267. , 272.5, 278. , 283.5],\n",
       "       [289. , 294.5, 300. , 305.5, 311. , 316.5],\n",
       "       [322. , 327.5, 333. , 338.5, 344. , 349.5],\n",
       "       [355. , 360.5, 366. , 371.5, 377. , 382.5]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.arange(58,387,step = 5.5, dtype=np.float32).reshape(10,6)\n",
    "inp.dtype, inp.shape\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6000.    ],\n",
       "       [6320.7   ],\n",
       "       [6641.4004],\n",
       "       [6962.1006],\n",
       "       [7282.801 ],\n",
       "       [7603.501 ],\n",
       "       [7924.201 ],\n",
       "       [8244.901 ],\n",
       "       [8565.602 ],\n",
       "       [8886.302 ]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = np.arange(6000,9000,step = 320.7,dtype=np.float32).reshape(10,1)\n",
    "actual.dtype, actual.shape\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= torch.from_numpy(inp)\n",
    "actual = torch.from_numpy(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataset.TensorDataset,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset gives us a tuple of inputs and targets\n",
    "ds = TensorDataset(inp, actual)\n",
    "\n",
    "# To divide the data into train and validation set we use random_split\n",
    "# [8,2] splits the data into training and validation dataset\n",
    "#tr_dataset, val_dataset = random_split(ds,[8,2])\n",
    "\n",
    "# Dataloader helps to split data into batches and shuffling the data\n",
    "train_loader = DataLoader(ds, shuffle=True)\n",
    "\n",
    "# datatypes of dataset an train_loader\n",
    "type(ds) , type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[157.0000, 162.5000, 168.0000, 173.5000, 179.0000, 184.5000]])\n",
      "tensor([[6962.1006]])\n",
      "tensor([[355.0000, 360.5000, 366.0000, 371.5000, 377.0000, 382.5000]])\n",
      "tensor([[8886.3018]])\n",
      "tensor([[322.0000, 327.5000, 333.0000, 338.5000, 344.0000, 349.5000]])\n",
      "tensor([[8565.6016]])\n",
      "tensor([[190.0000, 195.5000, 201.0000, 206.5000, 212.0000, 217.5000]])\n",
      "tensor([[7282.8008]])\n",
      "tensor([[256.0000, 261.5000, 267.0000, 272.5000, 278.0000, 283.5000]])\n",
      "tensor([[7924.2012]])\n",
      "tensor([[124.0000, 129.5000, 135.0000, 140.5000, 146.0000, 151.5000]])\n",
      "tensor([[6641.4004]])\n",
      "tensor([[289.0000, 294.5000, 300.0000, 305.5000, 311.0000, 316.5000]])\n",
      "tensor([[8244.9014]])\n",
      "tensor([[ 91.0000,  96.5000, 102.0000, 107.5000, 113.0000, 118.5000]])\n",
      "tensor([[6320.7002]])\n",
      "tensor([[58.0000, 63.5000, 69.0000, 74.5000, 80.0000, 85.5000]])\n",
      "tensor([[6000.]])\n",
      "tensor([[223.0000, 228.5000, 234.0000, 239.5000, 245.0000, 250.5000]])\n",
      "tensor([[7603.5010]])\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "#Iteration to see what a dataloader provides\n",
    "for data,label in train_loader:\n",
    "    print(data)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.linear automatically creates the matrix of wt abd bias\n",
    "# inputs are the number of columns in a tabular dataset\n",
    "# outputs can be the number of outputs\n",
    "input_size = 6\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size,output_size)\n",
    "\n",
    "# To look at the weight and bias use a parameter method\n",
    "list(model.parameters())\n",
    "\n",
    "# using the model to generate predictions\n",
    "predictions = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the loss function can be computed by directly using F.mse_loss\n",
    "loss = F.mse_loss(predictions,actual)\n",
    "\n",
    "# Models wts and bias can be updated automatically using a optimizer\n",
    "opt = torch.optim.SGD(model.parameters(),lr = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1025286.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1841393.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(9880742., grad_fn=<MseLossBackward0>)\n",
      "tensor(15824849., grad_fn=<MseLossBackward0>)\n",
      "tensor(12866921., grad_fn=<MseLossBackward0>)\n",
      "tensor(1403273.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(6889109., grad_fn=<MseLossBackward0>)\n",
      "tensor(301995.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(7300813., grad_fn=<MseLossBackward0>)\n",
      "tensor(6569762.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(8025976., grad_fn=<MseLossBackward0>)\n",
      "tensor(5783595., grad_fn=<MseLossBackward0>)\n",
      "tensor(4533934.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1997666.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(2004032.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(25588.5957, grad_fn=<MseLossBackward0>)\n",
      "tensor(379602.7812, grad_fn=<MseLossBackward0>)\n",
      "tensor(8045948.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(3089504.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(5630250.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(854514.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1580122.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(9193628., grad_fn=<MseLossBackward0>)\n",
      "tensor(128968.6641, grad_fn=<MseLossBackward0>)\n",
      "tensor(1670929.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1254736.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2535173., grad_fn=<MseLossBackward0>)\n",
      "tensor(528035.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(4599877., grad_fn=<MseLossBackward0>)\n",
      "tensor(1754838.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(2620946., grad_fn=<MseLossBackward0>)\n",
      "tensor(1700487.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1380360., grad_fn=<MseLossBackward0>)\n",
      "tensor(131735.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(394955.7812, grad_fn=<MseLossBackward0>)\n",
      "tensor(5332.2085, grad_fn=<MseLossBackward0>)\n",
      "tensor(6262877.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(576035.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(3134015., grad_fn=<MseLossBackward0>)\n",
      "tensor(3981889.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(31548.3848, grad_fn=<MseLossBackward0>)\n",
      "tensor(43048.5508, grad_fn=<MseLossBackward0>)\n",
      "tensor(1267220.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(2754742.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1623688.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(799257.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(2852575.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1129285., grad_fn=<MseLossBackward0>)\n",
      "tensor(5696063., grad_fn=<MseLossBackward0>)\n",
      "tensor(588753.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(85174.2031, grad_fn=<MseLossBackward0>)\n",
      "tensor(179881.5938, grad_fn=<MseLossBackward0>)\n",
      "tensor(553601.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1017881., grad_fn=<MseLossBackward0>)\n",
      "tensor(336.8515, grad_fn=<MseLossBackward0>)\n",
      "tensor(2092113.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1474040.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(689778.9375, grad_fn=<MseLossBackward0>)\n",
      "tensor(845809.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(614982.9375, grad_fn=<MseLossBackward0>)\n",
      "tensor(1456556.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(22297.1387, grad_fn=<MseLossBackward0>)\n",
      "tensor(299480.9688, grad_fn=<MseLossBackward0>)\n",
      "tensor(281979.6562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1540737.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(571171.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(68729.7422, grad_fn=<MseLossBackward0>)\n",
      "tensor(475251.3438, grad_fn=<MseLossBackward0>)\n",
      "tensor(1260493., grad_fn=<MseLossBackward0>)\n",
      "tensor(1407893.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(677600.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(1021236.5625, grad_fn=<MseLossBackward0>)\n",
      "tensor(570050.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(553846.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(9212.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(1395014.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(257966.7812, grad_fn=<MseLossBackward0>)\n",
      "tensor(594758.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(423076.5625, grad_fn=<MseLossBackward0>)\n",
      "tensor(1312152.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(126852.4922, grad_fn=<MseLossBackward0>)\n",
      "tensor(80669.0391, grad_fn=<MseLossBackward0>)\n",
      "tensor(1004843.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(25011.7012, grad_fn=<MseLossBackward0>)\n",
      "tensor(44540.3711, grad_fn=<MseLossBackward0>)\n",
      "tensor(207787.2969, grad_fn=<MseLossBackward0>)\n",
      "tensor(663612.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(87480.7734, grad_fn=<MseLossBackward0>)\n",
      "tensor(162032.9844, grad_fn=<MseLossBackward0>)\n",
      "tensor(297660.2812, grad_fn=<MseLossBackward0>)\n",
      "tensor(39607.0234, grad_fn=<MseLossBackward0>)\n",
      "tensor(642594.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(368919.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(668199.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(507437.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(117289.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(9352.4385, grad_fn=<MseLossBackward0>)\n",
      "tensor(306672.5938, grad_fn=<MseLossBackward0>)\n",
      "tensor(187027.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor(29711.7949, grad_fn=<MseLossBackward0>)\n",
      "tensor(581607.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(78159.0156, grad_fn=<MseLossBackward0>)\n",
      "tensor(33897.5195, grad_fn=<MseLossBackward0>)\n",
      "tensor(245715.7812, grad_fn=<MseLossBackward0>)\n",
      "tensor(27524.8828, grad_fn=<MseLossBackward0>)\n",
      "tensor(69760.2109, grad_fn=<MseLossBackward0>)\n",
      "tensor(294287.1875, grad_fn=<MseLossBackward0>)\n",
      "tensor(300696.4062, grad_fn=<MseLossBackward0>)\n",
      "tensor(2979.8379, grad_fn=<MseLossBackward0>)\n",
      "tensor(31175.6953, grad_fn=<MseLossBackward0>)\n",
      "tensor(181176.5938, grad_fn=<MseLossBackward0>)\n",
      "tensor(36089.6094, grad_fn=<MseLossBackward0>)\n",
      "tensor(12538.2041, grad_fn=<MseLossBackward0>)\n",
      "tensor(274085.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(39457.9023, grad_fn=<MseLossBackward0>)\n",
      "tensor(120143.1328, grad_fn=<MseLossBackward0>)\n",
      "tensor(57.8357, grad_fn=<MseLossBackward0>)\n",
      "tensor(205995.4219, grad_fn=<MseLossBackward0>)\n",
      "tensor(4740.1343, grad_fn=<MseLossBackward0>)\n",
      "tensor(69522.3438, grad_fn=<MseLossBackward0>)\n",
      "tensor(6058.8130, grad_fn=<MseLossBackward0>)\n",
      "tensor(182932.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(1775.9557, grad_fn=<MseLossBackward0>)\n",
      "tensor(88464.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(216301.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(164637.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(55552.7422, grad_fn=<MseLossBackward0>)\n",
      "tensor(142116.4844, grad_fn=<MseLossBackward0>)\n",
      "tensor(7043.0449, grad_fn=<MseLossBackward0>)\n",
      "tensor(63670.9609, grad_fn=<MseLossBackward0>)\n",
      "tensor(140223.1875, grad_fn=<MseLossBackward0>)\n",
      "tensor(38258.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(95772.7188, grad_fn=<MseLossBackward0>)\n",
      "tensor(73925.8359, grad_fn=<MseLossBackward0>)\n",
      "tensor(2549.5103, grad_fn=<MseLossBackward0>)\n",
      "tensor(117167.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(15822.8887, grad_fn=<MseLossBackward0>)\n",
      "tensor(70875.5391, grad_fn=<MseLossBackward0>)\n",
      "tensor(618.2313, grad_fn=<MseLossBackward0>)\n",
      "tensor(108391.4141, grad_fn=<MseLossBackward0>)\n",
      "tensor(5892.9609, grad_fn=<MseLossBackward0>)\n",
      "tensor(25092.8496, grad_fn=<MseLossBackward0>)\n",
      "tensor(18408.9785, grad_fn=<MseLossBackward0>)\n",
      "tensor(110383.8984, grad_fn=<MseLossBackward0>)\n",
      "tensor(3473.5713, grad_fn=<MseLossBackward0>)\n",
      "tensor(45406.4492, grad_fn=<MseLossBackward0>)\n",
      "tensor(68950.8594, grad_fn=<MseLossBackward0>)\n",
      "tensor(16553.3105, grad_fn=<MseLossBackward0>)\n",
      "tensor(103790., grad_fn=<MseLossBackward0>)\n",
      "tensor(1067.5791, grad_fn=<MseLossBackward0>)\n",
      "tensor(22363.9766, grad_fn=<MseLossBackward0>)\n",
      "tensor(82489.8438, grad_fn=<MseLossBackward0>)\n",
      "tensor(12118.9141, grad_fn=<MseLossBackward0>)\n",
      "tensor(10346.6045, grad_fn=<MseLossBackward0>)\n",
      "tensor(1892.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(6260.9976, grad_fn=<MseLossBackward0>)\n",
      "tensor(76310.8281, grad_fn=<MseLossBackward0>)\n",
      "tensor(24406.4336, grad_fn=<MseLossBackward0>)\n",
      "tensor(41235.1719, grad_fn=<MseLossBackward0>)\n",
      "tensor(50131.3867, grad_fn=<MseLossBackward0>)\n",
      "tensor(13964.5918, grad_fn=<MseLossBackward0>)\n",
      "tensor(20052.9395, grad_fn=<MseLossBackward0>)\n",
      "tensor(12091.7295, grad_fn=<MseLossBackward0>)\n",
      "tensor(44238.5352, grad_fn=<MseLossBackward0>)\n",
      "tensor(8997.7451, grad_fn=<MseLossBackward0>)\n",
      "tensor(16977.7852, grad_fn=<MseLossBackward0>)\n",
      "tensor(36235.7617, grad_fn=<MseLossBackward0>)\n",
      "tensor(39488.7539, grad_fn=<MseLossBackward0>)\n",
      "tensor(3396.1350, grad_fn=<MseLossBackward0>)\n",
      "tensor(6115.8052, grad_fn=<MseLossBackward0>)\n",
      "tensor(43898.4336, grad_fn=<MseLossBackward0>)\n",
      "tensor(7646.8535, grad_fn=<MseLossBackward0>)\n",
      "tensor(1718.7260, grad_fn=<MseLossBackward0>)\n",
      "tensor(9952.3027, grad_fn=<MseLossBackward0>)\n",
      "tensor(29442.5723, grad_fn=<MseLossBackward0>)\n",
      "tensor(434.0549, grad_fn=<MseLossBackward0>)\n",
      "tensor(8710.5039, grad_fn=<MseLossBackward0>)\n",
      "tensor(19052.3555, grad_fn=<MseLossBackward0>)\n",
      "tensor(6065.9604, grad_fn=<MseLossBackward0>)\n",
      "tensor(3110.9487, grad_fn=<MseLossBackward0>)\n",
      "tensor(109.2678, grad_fn=<MseLossBackward0>)\n",
      "tensor(5956.2520, grad_fn=<MseLossBackward0>)\n",
      "tensor(20417.4512, grad_fn=<MseLossBackward0>)\n",
      "tensor(1736.3010, grad_fn=<MseLossBackward0>)\n",
      "tensor(1516.0909, grad_fn=<MseLossBackward0>)\n",
      "tensor(7944.1362, grad_fn=<MseLossBackward0>)\n",
      "tensor(23.6515, grad_fn=<MseLossBackward0>)\n",
      "tensor(297.6636, grad_fn=<MseLossBackward0>)\n",
      "tensor(2779.6785, grad_fn=<MseLossBackward0>)\n",
      "tensor(7844.5273, grad_fn=<MseLossBackward0>)\n",
      "tensor(11828.2617, grad_fn=<MseLossBackward0>)\n",
      "tensor(644.9814, grad_fn=<MseLossBackward0>)\n",
      "tensor(3045.9836, grad_fn=<MseLossBackward0>)\n",
      "tensor(2930.4670, grad_fn=<MseLossBackward0>)\n",
      "tensor(3197.6594, grad_fn=<MseLossBackward0>)\n",
      "tensor(13068.6875, grad_fn=<MseLossBackward0>)\n",
      "tensor(2035.1200, grad_fn=<MseLossBackward0>)\n",
      "tensor(4551.7686, grad_fn=<MseLossBackward0>)\n",
      "tensor(13338.1074, grad_fn=<MseLossBackward0>)\n",
      "tensor(3780.8689, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "# Steps for nn implementation\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    for data,label in train_loader:\n",
    "        # preds\n",
    "        preds = model(data)\n",
    "        # calculate loss\n",
    "        loss = F.mse_loss(preds,label)\n",
    "        # gradient calculation\n",
    "        loss.backward()\n",
    "        # update wts and bias wrt loss\n",
    "        opt.step()\n",
    "        # gradients values are 0 again\n",
    "        opt.zero_grad()\n",
    "    if epoch%10==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5892.3145],\n",
       "        [6237.1528],\n",
       "        [6581.9897],\n",
       "        [6926.8257],\n",
       "        [7271.6616],\n",
       "        [7616.4976],\n",
       "        [7961.3335],\n",
       "        [8306.1768],\n",
       "        [8651.0127],\n",
       "        [8995.8486]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6000.0000],\n",
       "        [6320.7002],\n",
       "        [6641.4004],\n",
       "        [6962.1006],\n",
       "        [7282.8008],\n",
       "        [7603.5010],\n",
       "        [7924.2012],\n",
       "        [8244.9014],\n",
       "        [8565.6016],\n",
       "        [8886.3018]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "actual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6338af0e41d49d4a98e031440c1e6ba1901a7edf2fea4c64b6b2d7a0b97a576b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
