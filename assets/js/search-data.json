{
  
    
        "post0": {
            "title": "Gradient Descent from Scratch using Pytorch",
            "content": ". This notebook is an implementation of a gradient descent using PYTORCH . gradient_descent is the foundation of all deep learning problems. It is super critical tp understand this . import numpy as np import torch import pandas as pd from torch.utils.data import TensorDataset from torch.utils.data import DataLoader from torch.utils.data import random_split import torch.nn as nn import torch.nn.functional as F . inp = np.arange(58,387,step = 5.5, dtype=np.float32).reshape(10,6) inp . array([[ 58. , 63.5, 69. , 74.5, 80. , 85.5], [ 91. , 96.5, 102. , 107.5, 113. , 118.5], [124. , 129.5, 135. , 140.5, 146. , 151.5], [157. , 162.5, 168. , 173.5, 179. , 184.5], [190. , 195.5, 201. , 206.5, 212. , 217.5], [223. , 228.5, 234. , 239.5, 245. , 250.5], [256. , 261.5, 267. , 272.5, 278. , 283.5], [289. , 294.5, 300. , 305.5, 311. , 316.5], [322. , 327.5, 333. , 338.5, 344. , 349.5], [355. , 360.5, 366. , 371.5, 377. , 382.5]], dtype=float32) . . inp.dtype, inp.shape . (dtype(&#39;float32&#39;), (10, 6)) . . actual = np.arange(6000,9000,step = 320.7,dtype=np.float32).reshape(10,1) actual . array([[6000. ], [6320.7 ], [6641.4004], [6962.1006], [7282.801 ], [7603.501 ], [7924.201 ], [8244.901 ], [8565.602 ], [8886.302 ]], dtype=float32) . . actual.dtype, actual.shape . (dtype(&#39;float32&#39;), (10, 1)) . . inp= torch.from_numpy(inp) actual = torch.from_numpy(actual) . Convert to dataset and dataloader . ds = TensorDataset(inp, actual) # To divide the data into train and validation set we use random_split # [8,2] splits the data into training and validation dataset #tr_dataset, val_dataset = random_split(ds,[8,2]) # Dataloader helps to split data into batches and shuffling the data train_loader = DataLoader(ds, shuffle=True) # datatypes of dataset an train_loader type(ds) , type(train_loader) . (torch.utils.data.dataset.TensorDataset, torch.utils.data.dataloader.DataLoader) . A dataloader always gives a tuple of the training data and the label with it . #Iteration to see what a dataloader provides for data,label in train_loader: print(data) print(label) . tensor([[190.0000, 195.5000, 201.0000, 206.5000, 212.0000, 217.5000]]) tensor([[7282.8008]]) tensor([[322.0000, 327.5000, 333.0000, 338.5000, 344.0000, 349.5000]]) tensor([[8565.6016]]) tensor([[ 91.0000, 96.5000, 102.0000, 107.5000, 113.0000, 118.5000]]) tensor([[6320.7002]]) tensor([[256.0000, 261.5000, 267.0000, 272.5000, 278.0000, 283.5000]]) tensor([[7924.2012]]) tensor([[124.0000, 129.5000, 135.0000, 140.5000, 146.0000, 151.5000]]) tensor([[6641.4004]]) tensor([[289.0000, 294.5000, 300.0000, 305.5000, 311.0000, 316.5000]]) tensor([[8244.9014]]) tensor([[58.0000, 63.5000, 69.0000, 74.5000, 80.0000, 85.5000]]) tensor([[6000.]]) tensor([[355.0000, 360.5000, 366.0000, 371.5000, 377.0000, 382.5000]]) tensor([[8886.3018]]) tensor([[157.0000, 162.5000, 168.0000, 173.5000, 179.0000, 184.5000]]) tensor([[6962.1006]]) tensor([[223.0000, 228.5000, 234.0000, 239.5000, 245.0000, 250.5000]]) tensor([[7603.5010]]) . . Create a model using NN.Linear and pass the input through it to generate to produce the output . # inputs are the number of columns in a tabular dataset # outputs can be the number of outputs input_size = 6 output_size = 1 model = nn.Linear(input_size,output_size) # To look at the weight and bias use a parameter method list(model.parameters()) # using the model to generate predictions predictions = model(inp) . actual.shape, predictions.shape . (torch.Size([10, 1]), torch.Size([10, 1])) . Calculate the loss . # Now the loss function can be computed by directly using F.mse_loss loss = F.mse_loss(predictions,actual) # Models wts and bias can be updated automatically using a optimizer opt = torch.optim.SGD(model.parameters(),lr = 1e-6) opt . SGD ( Parameter Group 0 dampening: 0 lr: 1e-06 maximize: False momentum: 0 nesterov: False weight_decay: 0 ) . . Run the process of Gradient_Descent to find optimal weights and bias . # Steps for nn implementation num_epochs = 2000 for epoch in range(num_epochs): for data,label in train_loader: # preds preds = model(data) # calculate loss loss = F.mse_loss(preds,label) # gradient calculation loss.backward() # update wts and bias wrt loss opt.step() # gradients values are 0 again opt.zero_grad() if epoch%30==0: print(loss) . tensor(1276.5966, grad_fn=&lt;MseLossBackward0&gt;) tensor(2668.6709, grad_fn=&lt;MseLossBackward0&gt;) tensor(7169.9878, grad_fn=&lt;MseLossBackward0&gt;) tensor(469.6278, grad_fn=&lt;MseLossBackward0&gt;) tensor(6568.8335, grad_fn=&lt;MseLossBackward0&gt;) tensor(1190.0479, grad_fn=&lt;MseLossBackward0&gt;) tensor(325.2668, grad_fn=&lt;MseLossBackward0&gt;) tensor(5114.6245, grad_fn=&lt;MseLossBackward0&gt;) tensor(1722.8174, grad_fn=&lt;MseLossBackward0&gt;) tensor(1077.5576, grad_fn=&lt;MseLossBackward0&gt;) tensor(160.6259, grad_fn=&lt;MseLossBackward0&gt;) tensor(375.2014, grad_fn=&lt;MseLossBackward0&gt;) tensor(4391.7803, grad_fn=&lt;MseLossBackward0&gt;) tensor(2078.6121, grad_fn=&lt;MseLossBackward0&gt;) tensor(2570.0146, grad_fn=&lt;MseLossBackward0&gt;) tensor(725.0766, grad_fn=&lt;MseLossBackward0&gt;) tensor(2081.1060, grad_fn=&lt;MseLossBackward0&gt;) tensor(873.6527, grad_fn=&lt;MseLossBackward0&gt;) tensor(83.2389, grad_fn=&lt;MseLossBackward0&gt;) tensor(1916.8406, grad_fn=&lt;MseLossBackward0&gt;) tensor(719.1458, grad_fn=&lt;MseLossBackward0&gt;) tensor(1902.9702, grad_fn=&lt;MseLossBackward0&gt;) tensor(42.3262, grad_fn=&lt;MseLossBackward0&gt;) tensor(763.7343, grad_fn=&lt;MseLossBackward0&gt;) tensor(365.8590, grad_fn=&lt;MseLossBackward0&gt;) tensor(112.0207, grad_fn=&lt;MseLossBackward0&gt;) tensor(925.1934, grad_fn=&lt;MseLossBackward0&gt;) tensor(158.8609, grad_fn=&lt;MseLossBackward0&gt;) tensor(363.2858, grad_fn=&lt;MseLossBackward0&gt;) tensor(0.4345, grad_fn=&lt;MseLossBackward0&gt;) tensor(453.9729, grad_fn=&lt;MseLossBackward0&gt;) tensor(123.0280, grad_fn=&lt;MseLossBackward0&gt;) tensor(30.7137, grad_fn=&lt;MseLossBackward0&gt;) tensor(350.9402, grad_fn=&lt;MseLossBackward0&gt;) tensor(89.2324, grad_fn=&lt;MseLossBackward0&gt;) tensor(296.6872, grad_fn=&lt;MseLossBackward0&gt;) tensor(19.0595, grad_fn=&lt;MseLossBackward0&gt;) tensor(20.8788, grad_fn=&lt;MseLossBackward0&gt;) tensor(83.6939, grad_fn=&lt;MseLossBackward0&gt;) tensor(173.1255, grad_fn=&lt;MseLossBackward0&gt;) tensor(155.1168, grad_fn=&lt;MseLossBackward0&gt;) tensor(125.8494, grad_fn=&lt;MseLossBackward0&gt;) tensor(179.1650, grad_fn=&lt;MseLossBackward0&gt;) tensor(92.3213, grad_fn=&lt;MseLossBackward0&gt;) tensor(23.1508, grad_fn=&lt;MseLossBackward0&gt;) tensor(34.4067, grad_fn=&lt;MseLossBackward0&gt;) tensor(54.5636, grad_fn=&lt;MseLossBackward0&gt;) tensor(18.6440, grad_fn=&lt;MseLossBackward0&gt;) tensor(145.8339, grad_fn=&lt;MseLossBackward0&gt;) tensor(47.0510, grad_fn=&lt;MseLossBackward0&gt;) tensor(21.9315, grad_fn=&lt;MseLossBackward0&gt;) tensor(59.0601, grad_fn=&lt;MseLossBackward0&gt;) tensor(5.9200, grad_fn=&lt;MseLossBackward0&gt;) tensor(27.8658, grad_fn=&lt;MseLossBackward0&gt;) tensor(23.2401, grad_fn=&lt;MseLossBackward0&gt;) tensor(2.5090, grad_fn=&lt;MseLossBackward0&gt;) tensor(5.5459, grad_fn=&lt;MseLossBackward0&gt;) tensor(12.6879, grad_fn=&lt;MseLossBackward0&gt;) tensor(32.2145, grad_fn=&lt;MseLossBackward0&gt;) tensor(31.1918, grad_fn=&lt;MseLossBackward0&gt;) tensor(1.8559, grad_fn=&lt;MseLossBackward0&gt;) tensor(14.7257, grad_fn=&lt;MseLossBackward0&gt;) tensor(8.5545, grad_fn=&lt;MseLossBackward0&gt;) tensor(6.6442, grad_fn=&lt;MseLossBackward0&gt;) tensor(18.0003, grad_fn=&lt;MseLossBackward0&gt;) tensor(11.6892, grad_fn=&lt;MseLossBackward0&gt;) tensor(1.4676, grad_fn=&lt;MseLossBackward0&gt;) . . Now we can compare the results obtained by the model after the weights are updated and gradient_descent is run . We can see that we are quite close to the actual value . # predictions after the weights are updated model(inp) . tensor([[5996.6123], [6318.0566], [6639.5049], [6960.9502], [7282.3916], [7603.8350], [7925.2881], [8246.7295], [8568.1748], [8889.6162]], grad_fn=&lt;AddmmBackward0&gt;) . . # Actual values from the model actual . tensor([[6000.0000], [6320.7002], [6641.4004], [6962.1006], [7282.8008], [7603.5010], [7924.2012], [8244.9014], [8565.6016], [8886.3018]]) . .",
            "url": "https://nakulpyasi.github.io/Unified-AI/jupyter/2022/08/15/NN.html",
            "relUrl": "/jupyter/2022/08/15/NN.html",
            "date": " • Aug 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nakulpyasi.github.io/Unified-AI/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nakulpyasi.github.io/Unified-AI/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nakulpyasi.github.io/Unified-AI/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nakulpyasi.github.io/Unified-AI/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}