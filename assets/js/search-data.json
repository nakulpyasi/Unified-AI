{
  
    
        "post0": {
            "title": "MNIST dataset - NN example(Pytorch)",
            "content": ". import torch import torchvision from torchvision.datasets import MNIST from torchvision.transforms import transforms import matplotlib.pyplot as plt from torch.utils.data import random_split from torch.utils.data import DataLoader import torch.nn as nn import torch.nn.functional as F . # this line of code will download the images(60000) in the defined root folder # download = True - when ran once will nor re-download all the images if it finds the images in the root path # train = True means it is the training set dataset = MNIST(root=&#39;Deep_Learning_Explorations/data/&#39;,train = True, download=True,transform=transforms.ToTensor()) # Dataset is a tuple of image and the label # indexing the dataset will show us that # shape of thwe image is 1*28*28 # 28 * 28 are the pixel values ranging from values 0 to 1 # this image has a just one channel as it is a gray scale image type(dataset[0][0]), type(dataset[0][1]), dataset[0][0].shape . (torch.Tensor, int, torch.Size([1, 28, 28])) . plt.imshow(dataset[0][0], cmap = &#39;gray&#39;); . Split Dataset - Training and Validation . # Split the dataset into a training and the validation set tr_data, val_data = random_split(dataset,[50000,10000]) len(tr_data), len(val_data) . (50000, 10000) . . Training and Validation Dataloader . # Dataloader helps in converting the dataset into batches of data by describing the batch_size tr_loader = DataLoader(tr_data,batch_size=200,shuffle=True) val_loader = DataLoader(val_data,batch_size=200) . # We can see that the data is a batch of 128 images and 120 labels for data,label in tr_loader: print(data.shape) print(len(label)) break . torch.Size([200, 1, 28, 28]) 200 . . # x@w.t()+ bias - Use the same equation model = nn.Linear(in_features= 784,out_features=10) # A model will randomly initialize the parameters(wt&#39;s and biases) model.weight.shape, model.bias.shape . (torch.Size([10, 784]), torch.Size([10])) . . for img,label in tr_loader: # shape of the batch in the beggining print(img.shape) #this functionality can be added inside the model in the forward method img = img.reshape(-1,28*28) # shape after reshaping into a vector of 784 elements print(img.shape) # applying the model to the reshaped img pred = model(img) print(model(img).shape) # As we can see that the model has outputted 10 probabilities which is a prob for all elements from 0-9 break . torch.Size([200, 1, 28, 28]) torch.Size([200, 784]) torch.Size([200, 10]) . . print(pred) pred.shape . tensor([[-0.1780, -0.2030, -0.1832, ..., -0.0791, 0.1601, -0.0289], [-0.0835, -0.3761, -0.2121, ..., -0.2008, -0.2714, -0.2222], [-0.0419, -0.1072, -0.0031, ..., -0.1503, 0.0071, -0.1969], ..., [-0.1106, -0.1116, 0.1379, ..., -0.0675, -0.0677, 0.0206], [-0.0918, -0.1213, -0.0014, ..., -0.1266, 0.1137, 0.3449], [-0.2163, 0.0296, 0.0380, ..., -0.0783, -0.1645, -0.1849]], grad_fn=&lt;AddmmBackward0&gt;) . torch.Size([200, 10]) . . Additional functionality to our NN model . To add additional functionality to the NN model we need to create a MnistModel class and inheret the nn.Module. Addiditional functionality in this is the step of reshaping the batch of data passing through the model to a vector of 784 pixels . class MnistModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(784, 10) def forward(self, xb): xb = xb.reshape(-1, 784) out = self.linear(xb) return out model = MNIST() . model.layer.weight.shape, model.layer.bias.shape . (torch.Size([10, 784]), torch.Size([10])) . Sofmax . Softmax converts a vector of K real numbers into a probability distribution of K possible outcomes . It is basically calculated by taking the exponent of preds and dividing by their sum to make sure its sum is 1 . for dta,label in tr_loader: pred = model(dta) print(pred.shape) print(label.shape) break # We will apply softmax now - which converts the probability b/w 0 and 1 and the sum is 1 torch.sum(F.softmax(pred[0])).item() # Applying softmax on the whole batch pred_s = F.softmax(pred,dim=1) # torch amx function gives us the index of the max probability as well as the probability index_prob,prob = torch.max(pred_s,dim=1) index_prob.shape,prob.shape . torch.Size([200, 10]) torch.Size([200]) . /var/folders/j4/0sh22ln930vdhyh1wkttl89m0000gp/T/ipykernel_24095/3939508011.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. torch.sum(F.softmax(pred[0])).item() . (torch.Size([200]), torch.Size([200])) . Accuracy . The predictions are converted into probabilities and the highest probability is calculated using the max function . The index of the highest probability is then compared to the actual label and the accuracy % is calculated by divding the correct predictions with the total images . def metric_acc(out,label): index_prob,prob = torch.max(pred_s,dim=1) return torch.sum(prob==label)/prob.numel() # We will get the same value even if we do not apply the softmax as # e^x is an increasing function, i.e., if y1 &gt; y2, then e^y1 &gt; e^y2. The same holds after averaging out the values to get the softmax. metric_acc(prob,label) . tensor(0.0700) . # loss function will be cross_entropy loss = F.cross_entropy(pred, label) print(loss) . tensor(2.3289, grad_fn=&lt;NllLossBackward0&gt;) . Model Training . This fit function is the training step. This training step invovles training the model on the training dataloader, calculating the loss, calculating the gradient for the train loader and updating the weights and reseting the gradient at the end. . For the second part of the loop - we validate the model on the validation dataloader. The steps include calculating the loss and accuracy after each epoch and printing them at the end. We can notice that the loss and the accuracy on the validation set improves after each epoch . def fit(epochs,learning_rate,model,train_loader,val_loader,opt_func=torch.optim.SGD): optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate , momentum=0.9) out_lst = [] for epoch in range(epochs): # Training step on the training dataloader for batch in tr_loader: #extract batch of images and label img,label = batch #calculate prediction using the MNISTMODEL class initialized above pred = model(img) #Since this is a multi-label image classification model- the loss function is cross entropy loss = F.cross_entropy(pred,label) # In this step we calculate the gradient of the loss function with respect to the parameters or 784 pixels in this case loss.backward() # In this step we update the weights optimizer.step() # We make the gradient zero again so that now the gradients are not calculated untill the training is not done optimizer.zero_grad() # Validation on the validation dataloader for batch_val in val_loader: img_val,label_val = batch_val pred_val = model(img_val) # loss is computed loss_val = F.cross_entropy(pred_val,label_val) # Accuracy is computed acc_val = metric_acc(pred_val,label_val) out_lst.append({&#39;Epoch&#39;:epoch,&#39;val_loss&#39;: loss_val, &#39;val_acc&#39;: acc_val}) # Accuracies and loss are stacked together for each epoch, mean is calculated and the results are printed val_loss_epoch = torch.stack([dct[&#39;val_loss&#39;] for dct in out_lst]).mean() val_acc_epoch = torch.stack([dct[&#39;val_acc&#39;] for dct in out_lst]).mean() print(&#39;Epoch: {0}, loss: {1}, accuracy_val: {2}&#39;.format(epoch,val_loss_epoch,val_acc_epoch)) model = MnistModel() . We can clearly see that with each epoch the loss and accuracies improve. You can try to use different number of epochs and learning rate to see if you can improve the accuracy . fit(20,learning_rate=0.005,model= model,train_loader = tr_loader,val_loader = val_loader) . Epoch: 0, loss: 0.5632244348526001, accuracy_val: 0.861500084400177 Epoch: 1, loss: 0.5117447376251221, accuracy_val: 0.8712499737739563 Epoch: 2, loss: 0.4806540310382843, accuracy_val: 0.8765000700950623 Epoch: 3, loss: 0.45891621708869934, accuracy_val: 0.8803249597549438 Epoch: 4, loss: 0.44262486696243286, accuracy_val: 0.8833799362182617 Epoch: 5, loss: 0.4297642111778259, accuracy_val: 0.885816752910614 Epoch: 6, loss: 0.4192468225955963, accuracy_val: 0.8878857493400574 Epoch: 7, loss: 0.41042187809944153, accuracy_val: 0.889549970626831 Epoch: 8, loss: 0.4029468595981598, accuracy_val: 0.891033411026001 Epoch: 9, loss: 0.3964667320251465, accuracy_val: 0.8922899961471558 Epoch: 10, loss: 0.3907521069049835, accuracy_val: 0.8933908939361572 Epoch: 11, loss: 0.385611355304718, accuracy_val: 0.8943833708763123 Epoch: 12, loss: 0.38104525208473206, accuracy_val: 0.8952845931053162 Epoch: 13, loss: 0.37690234184265137, accuracy_val: 0.8960857391357422 Epoch: 14, loss: 0.37316834926605225, accuracy_val: 0.8968601226806641 Epoch: 15, loss: 0.3697192370891571, accuracy_val: 0.8975686430931091 Epoch: 16, loss: 0.36654046177864075, accuracy_val: 0.8981589078903198 Epoch: 17, loss: 0.3636338710784912, accuracy_val: 0.8987833857536316 Epoch: 18, loss: 0.3609280586242676, accuracy_val: 0.8993525505065918 Epoch: 19, loss: 0.35844382643699646, accuracy_val: 0.899869978427887 . . Testing on the test set . img_test = MNIST(root=&#39;Deep_Learning_Explorations/data/&#39;,train = False,transform=transforms.ToTensor()) . # gray scale has just 1 channnel # 1*28*28, 1 here specifies that image has just one channel img, label = img_test[12] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Shape:&#39;, img.shape) print(&#39;Label:&#39;, label) . Shape: torch.Size([1, 28, 28]) Label: 9 . Prediction Function on the test set . def pred_function(img,model): img.shape inp = img.unsqueeze(0) out = model(inp) prob , preds = torch.max(out,dim=1) return preds[0].item() . img, label = img_test[200] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;, Predicted:&#39;, pred_function(img, model)) . Label: 3 , Predicted: 3 . img, label = img_test[123] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;, Predicted:&#39;, pred_function(img, model)) . Label: 6 , Predicted: 6 . Testing on all images - the test set . test_loader = DataLoader(img_test, batch_size=20) # pass the batches to the model and calculate model accuracy and loss def test_set_preds(test_loader,model): out_lst = [] for i, batch_val in enumerate(test_loader): img_val,label_val = batch_val pred_val = model(img_val) # loss is computed for that batch loss_val = F.cross_entropy(pred_val,label_val) # Accuracy is computed for that batch acc_val = metric_acc(pred_val,label_val) out_lst.append({&#39;val_loss&#39;: loss_val, &#39;val_acc&#39;: acc_val}) #Accuracies and loss are stacked together for each epoch, mean is calculated and the results are printed final_loss = torch.stack([dct[&#39;val_loss&#39;] for dct in out_lst]).mean() final_acc= torch.stack([dct[&#39;val_acc&#39;] for dct in out_lst]).mean() return &#39;loss: {0}, accuracy_val: {1}&#39;.format(final_loss,final_acc) . test_set_preds(test_loader,model) . &#39;loss: 0.297660231590271, accuracy_val: 0.9175999164581299&#39; .",
            "url": "https://nakulpyasi.github.io/Unified-AI/jupyter/2022/08/19/MNIST-Pytorch.html",
            "relUrl": "/jupyter/2022/08/19/MNIST-Pytorch.html",
            "date": " • Aug 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Gradient Descent from Scratch using Pytorch",
            "content": ". This notebook is an implementation of a gradient descent using PYTORCH . gradient_descent is the foundation of all deep learning problems. It is super critical tp understand this . import numpy as np import torch import pandas as pd from torch.utils.data import TensorDataset from torch.utils.data import DataLoader from torch.utils.data import random_split import torch.nn as nn import torch.nn.functional as F . inp = np.arange(58,387,step = 5.5, dtype=np.float32).reshape(10,6) inp . array([[ 58. , 63.5, 69. , 74.5, 80. , 85.5], [ 91. , 96.5, 102. , 107.5, 113. , 118.5], [124. , 129.5, 135. , 140.5, 146. , 151.5], [157. , 162.5, 168. , 173.5, 179. , 184.5], [190. , 195.5, 201. , 206.5, 212. , 217.5], [223. , 228.5, 234. , 239.5, 245. , 250.5], [256. , 261.5, 267. , 272.5, 278. , 283.5], [289. , 294.5, 300. , 305.5, 311. , 316.5], [322. , 327.5, 333. , 338.5, 344. , 349.5], [355. , 360.5, 366. , 371.5, 377. , 382.5]], dtype=float32) . . inp.dtype, inp.shape . (dtype(&#39;float32&#39;), (10, 6)) . . actual = np.arange(6000,9000,step = 320.7,dtype=np.float32).reshape(10,1) actual . array([[6000. ], [6320.7 ], [6641.4004], [6962.1006], [7282.801 ], [7603.501 ], [7924.201 ], [8244.901 ], [8565.602 ], [8886.302 ]], dtype=float32) . . actual.dtype, actual.shape . (dtype(&#39;float32&#39;), (10, 1)) . . inp= torch.from_numpy(inp) actual = torch.from_numpy(actual) . Convert to dataset and dataloader . ds = TensorDataset(inp, actual) # To divide the data into train and validation set we use random_split # [8,2] splits the data into training and validation dataset #tr_dataset, val_dataset = random_split(ds,[8,2]) # Dataloader helps to split data into batches and shuffling the data train_loader = DataLoader(ds, shuffle=True) # datatypes of dataset an train_loader type(ds) , type(train_loader) . (torch.utils.data.dataset.TensorDataset, torch.utils.data.dataloader.DataLoader) . A dataloader always gives a tuple of the training data and the label with it . #Iteration to see what a dataloader provides for data,label in train_loader: print(data) print(label) . tensor([[190.0000, 195.5000, 201.0000, 206.5000, 212.0000, 217.5000]]) tensor([[7282.8008]]) tensor([[322.0000, 327.5000, 333.0000, 338.5000, 344.0000, 349.5000]]) tensor([[8565.6016]]) tensor([[ 91.0000, 96.5000, 102.0000, 107.5000, 113.0000, 118.5000]]) tensor([[6320.7002]]) tensor([[256.0000, 261.5000, 267.0000, 272.5000, 278.0000, 283.5000]]) tensor([[7924.2012]]) tensor([[124.0000, 129.5000, 135.0000, 140.5000, 146.0000, 151.5000]]) tensor([[6641.4004]]) tensor([[289.0000, 294.5000, 300.0000, 305.5000, 311.0000, 316.5000]]) tensor([[8244.9014]]) tensor([[58.0000, 63.5000, 69.0000, 74.5000, 80.0000, 85.5000]]) tensor([[6000.]]) tensor([[355.0000, 360.5000, 366.0000, 371.5000, 377.0000, 382.5000]]) tensor([[8886.3018]]) tensor([[157.0000, 162.5000, 168.0000, 173.5000, 179.0000, 184.5000]]) tensor([[6962.1006]]) tensor([[223.0000, 228.5000, 234.0000, 239.5000, 245.0000, 250.5000]]) tensor([[7603.5010]]) . . Create a model using NN.Linear and pass the input through it to generate to produce the output . # inputs are the number of columns in a tabular dataset # outputs can be the number of outputs input_size = 6 output_size = 1 model = nn.Linear(input_size,output_size) # To look at the weight and bias use a parameter method list(model.parameters()) # using the model to generate predictions predictions = model(inp) . actual.shape, predictions.shape . (torch.Size([10, 1]), torch.Size([10, 1])) . Calculate the loss . # Now the loss function can be computed by directly using F.mse_loss loss = F.mse_loss(predictions,actual) # Models wts and bias can be updated automatically using a optimizer opt = torch.optim.SGD(model.parameters(),lr = 1e-6) opt . SGD ( Parameter Group 0 dampening: 0 lr: 1e-06 maximize: False momentum: 0 nesterov: False weight_decay: 0 ) . . Run the process of Gradient_Descent to find optimal weights and bias . # Steps for nn implementation num_epochs = 2000 for epoch in range(num_epochs): for data,label in train_loader: # preds preds = model(data) # calculate loss loss = F.mse_loss(preds,label) # gradient calculation loss.backward() # update wts and bias wrt loss opt.step() # gradients values are 0 again opt.zero_grad() if epoch%30==0: print(loss) . tensor(1276.5966, grad_fn=&lt;MseLossBackward0&gt;) tensor(2668.6709, grad_fn=&lt;MseLossBackward0&gt;) tensor(7169.9878, grad_fn=&lt;MseLossBackward0&gt;) tensor(469.6278, grad_fn=&lt;MseLossBackward0&gt;) tensor(6568.8335, grad_fn=&lt;MseLossBackward0&gt;) tensor(1190.0479, grad_fn=&lt;MseLossBackward0&gt;) tensor(325.2668, grad_fn=&lt;MseLossBackward0&gt;) tensor(5114.6245, grad_fn=&lt;MseLossBackward0&gt;) tensor(1722.8174, grad_fn=&lt;MseLossBackward0&gt;) tensor(1077.5576, grad_fn=&lt;MseLossBackward0&gt;) tensor(160.6259, grad_fn=&lt;MseLossBackward0&gt;) tensor(375.2014, grad_fn=&lt;MseLossBackward0&gt;) tensor(4391.7803, grad_fn=&lt;MseLossBackward0&gt;) tensor(2078.6121, grad_fn=&lt;MseLossBackward0&gt;) tensor(2570.0146, grad_fn=&lt;MseLossBackward0&gt;) tensor(725.0766, grad_fn=&lt;MseLossBackward0&gt;) tensor(2081.1060, grad_fn=&lt;MseLossBackward0&gt;) tensor(873.6527, grad_fn=&lt;MseLossBackward0&gt;) tensor(83.2389, grad_fn=&lt;MseLossBackward0&gt;) tensor(1916.8406, grad_fn=&lt;MseLossBackward0&gt;) tensor(719.1458, grad_fn=&lt;MseLossBackward0&gt;) tensor(1902.9702, grad_fn=&lt;MseLossBackward0&gt;) tensor(42.3262, grad_fn=&lt;MseLossBackward0&gt;) tensor(763.7343, grad_fn=&lt;MseLossBackward0&gt;) tensor(365.8590, grad_fn=&lt;MseLossBackward0&gt;) tensor(112.0207, grad_fn=&lt;MseLossBackward0&gt;) tensor(925.1934, grad_fn=&lt;MseLossBackward0&gt;) tensor(158.8609, grad_fn=&lt;MseLossBackward0&gt;) tensor(363.2858, grad_fn=&lt;MseLossBackward0&gt;) tensor(0.4345, grad_fn=&lt;MseLossBackward0&gt;) tensor(453.9729, grad_fn=&lt;MseLossBackward0&gt;) tensor(123.0280, grad_fn=&lt;MseLossBackward0&gt;) tensor(30.7137, grad_fn=&lt;MseLossBackward0&gt;) tensor(350.9402, grad_fn=&lt;MseLossBackward0&gt;) tensor(89.2324, grad_fn=&lt;MseLossBackward0&gt;) tensor(296.6872, grad_fn=&lt;MseLossBackward0&gt;) tensor(19.0595, grad_fn=&lt;MseLossBackward0&gt;) tensor(20.8788, grad_fn=&lt;MseLossBackward0&gt;) tensor(83.6939, grad_fn=&lt;MseLossBackward0&gt;) tensor(173.1255, grad_fn=&lt;MseLossBackward0&gt;) tensor(155.1168, grad_fn=&lt;MseLossBackward0&gt;) tensor(125.8494, grad_fn=&lt;MseLossBackward0&gt;) tensor(179.1650, grad_fn=&lt;MseLossBackward0&gt;) tensor(92.3213, grad_fn=&lt;MseLossBackward0&gt;) tensor(23.1508, grad_fn=&lt;MseLossBackward0&gt;) tensor(34.4067, grad_fn=&lt;MseLossBackward0&gt;) tensor(54.5636, grad_fn=&lt;MseLossBackward0&gt;) tensor(18.6440, grad_fn=&lt;MseLossBackward0&gt;) tensor(145.8339, grad_fn=&lt;MseLossBackward0&gt;) tensor(47.0510, grad_fn=&lt;MseLossBackward0&gt;) tensor(21.9315, grad_fn=&lt;MseLossBackward0&gt;) tensor(59.0601, grad_fn=&lt;MseLossBackward0&gt;) tensor(5.9200, grad_fn=&lt;MseLossBackward0&gt;) tensor(27.8658, grad_fn=&lt;MseLossBackward0&gt;) tensor(23.2401, grad_fn=&lt;MseLossBackward0&gt;) tensor(2.5090, grad_fn=&lt;MseLossBackward0&gt;) tensor(5.5459, grad_fn=&lt;MseLossBackward0&gt;) tensor(12.6879, grad_fn=&lt;MseLossBackward0&gt;) tensor(32.2145, grad_fn=&lt;MseLossBackward0&gt;) tensor(31.1918, grad_fn=&lt;MseLossBackward0&gt;) tensor(1.8559, grad_fn=&lt;MseLossBackward0&gt;) tensor(14.7257, grad_fn=&lt;MseLossBackward0&gt;) tensor(8.5545, grad_fn=&lt;MseLossBackward0&gt;) tensor(6.6442, grad_fn=&lt;MseLossBackward0&gt;) tensor(18.0003, grad_fn=&lt;MseLossBackward0&gt;) tensor(11.6892, grad_fn=&lt;MseLossBackward0&gt;) tensor(1.4676, grad_fn=&lt;MseLossBackward0&gt;) . . Now we can compare the results obtained by the model after the weights are updated and gradient_descent is run . We can see that we are quite close to the actual value . # predictions after the weights are updated model(inp) . tensor([[5996.6123], [6318.0566], [6639.5049], [6960.9502], [7282.3916], [7603.8350], [7925.2881], [8246.7295], [8568.1748], [8889.6162]], grad_fn=&lt;AddmmBackward0&gt;) . . # Actual values from the model actual . tensor([[6000.0000], [6320.7002], [6641.4004], [6962.1006], [7282.8008], [7603.5010], [7924.2012], [8244.9014], [8565.6016], [8886.3018]]) . .",
            "url": "https://nakulpyasi.github.io/Unified-AI/jupyter/2022/08/15/NN.html",
            "relUrl": "/jupyter/2022/08/15/NN.html",
            "date": " • Aug 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nakulpyasi.github.io/Unified-AI/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nakulpyasi.github.io/Unified-AI/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nakulpyasi.github.io/Unified-AI/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}